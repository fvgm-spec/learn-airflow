## Learn Airflow

### Dags folder

Inside this folder you will find some sample dags made to be executed in te Airflow server, you just need to create a conda environment following the steps detailed in the main [README file](https://github.com/fvgm-spec/learn-airflow/blob/main/README.md) of this repo.

<details>
<summary>

### **hello_world_dag.py**

</summary>

This dag corresponds to the `hello_world` sample in every programming languague, which is the first code executed to start learning the basics.

Here a basic DAG is created that runs in an hourly basis by executing a `heloworld` function.

</details>

<details>
<summary>

### **reading_pageviews.py**

</summary>

This [sample DAG](https://github.com/BasPH/data-pipelines-with-apache-airflow/blob/master/chapter04/dags/listing_4_20.py) is introduced in the book from *Bas Harenslak & Julian de Ruiter - Data Pipelines with Airflow* aka. The Airflow Bible.

This dag basically performs the following steps:

This Apache Airflow DAG, named "read_wiki_pages," is designed to automate the process of fetching pageview data from Wikipedia, processing it, and then storing it in a PostgreSQL database. Here's a step-by-step explanation of the DAG's functionality:

1. **Import Necessary Modules and Libraries**:
   - The code starts by importing various modules and libraries, including modules for making HTTP requests (`urllib.request`), date handling (`airflow.utils.dates`), and Airflow components (`DAG`, `PythonOperator`, `BashOperator`, `PostgresOperator`).

2. **Creating an Airflow DAG**:
   - A DAG object is created with several attributes and settings:
     - `dag_id`: This is a unique identifier for the DAG, set as "read_wiki_pages."
     - `start_date`: The DAG's start date is set to one day ago from the current date using `airflow.utils.dates.days_ago(1)`.
     - `schedule_interval`: The DAG is scheduled to run hourly using `"@hourly"`.
     - `template_searchpath`: This specifies the search path for template files. In this case, it's set to "/tmp."
     - `max_active_runs`: It's set to 1, indicating that only one instance of this DAG can run concurrently.

3. **Defining Functions**:
   - Two Python functions are defined within the DAG:
     - `_get_data`: This function retrieves data from a URL that corresponds to Wikipedia pageviews for a specific date and time. It constructs the URL dynamically based on input arguments and saves the data to a specified output file.
     - `_fetch_pageviews`: This function processes the downloaded pageview data, extracts relevant information, and prepares SQL queries to insert the data into a PostgreSQL database.

4. **Creating Tasks**:
   - Several Airflow tasks are created within the DAG:
     - `get_data`: A PythonOperator task that executes the `_get_data` function. It retrieves pageview data from a URL based on the execution date and saves it to a gzip-compressed file.
     - `extract_gz`: A BashOperator task that runs the "gunzip" command to decompress the downloaded data file.
     - `fetch_pageviews`: Another PythonOperator task that executes the `_fetch_pageviews` function. It processes the extracted pageview data and prepares SQL queries.
     - `write_to_postgres`: A PostgresOperator task that writes data to a PostgreSQL database. It uses the SQL queries generated by the `_fetch_pageviews` function.

5. **Task Dependencies**:
   - The `>>` operator is used to define the execution order (task dependencies) within the DAG. The tasks are sequenced as follows:
     - `get_data` >> `extract_gz` >> `fetch_pageviews` >> `write_to_postgres`

This DAG is designed to automate the process of fetching, processing, and storing Wikipedia pageview data, making it a useful tool for data pipeline automation in an Airflow environment.

</details>

<details>
<summary>

### **getting_movies.py**

This DAG is designed to automate the process of fetching information about trending movies from The Movie Database (TMDb) API, including movie posters. Here's a step-by-step explanation of what the DAG does:

 <p>
<div class="column">
    <img src="img/1LhziFOxOyAiDjaTw2ru5iYSrYM.jpg" style="height: 15rem"/>
    <img src="img/4Q56IatK3JDRkYWXTs29vp7RECZ.jpg" style="height: 15rem"/>
    <img src="img/29WJ7dOHt48AtXK1J1rONEEvIMN.jpg" style="height: 15rem"/>
    <img src="img/51tqzRtKMMZEYUpSYkrUE7v9ehm.jpg" style="height: 15rem"/>
    <img src="img/A4LTXT8MMZIr4aIwhE4qbGFivBo.jpg" style="height: 15rem"/>
    <img src="img/AdcXd3zgbtoo2EDn3ymp35gdZoH.jpg" style="height: 15rem"/>
    <img src="img/aQPeznSu7XDTrrdCtT5eLiu52Yu.jpg" style="height: 15rem"/>
    <img src="img/cAn1tvopkqATQ0AfV4LK08ReAvp.jpg" style="height: 15rem"/>
    <img src="img/dfS5qHWFuXyZQnwYREwb7N4qU5p.jpg" style="height: 15rem"/>
    <img src="img/efPRTvNLdwnMVy7E4rZmEma7sja.jpg" style="height: 15rem"/>
    <img src="img/ehGIDAMaYy6Eg0o8ga0oqflDjqW.jpg" style="height: 15rem"/>
    <img src="img/evgEvYCUGidCyUCxA77EUJiWpVQ.jpg" style="height: 15rem"/>
  </div>
 </p>

Step-by-step explanation of the DAG's functionality:

**Import Necessary Modules and Libraries**:
   - The code starts by importing various modules and libraries, including `requests` for making HTTP requests to the TMDb API, `pathlib` for handling file paths, and Airflow components (`DAG`, `BashOperator`, `PythonOperator`, `Variable`).

**Set Up API Token and Folder Names**:
   - The `TMDB_API_TOKEN` variable is set with the name of the Airflow Variable that stores the TMDb API token.
   - The current date and time are obtained using `datetime.now()`, and the date is formatted as "MM-DD" to create a folder name.
   
**Creating an Airflow DAG**:
   - A DAG object is created with several attributes and settings:
     - `dag_id`: This is a unique identifier for the DAG, set as "get_trending_movies."
     - `start_date`: The DAG's start date is set to one day ago from the current date using `airflow.utils.dates.days_ago(1)`.
     - `schedule_interval`: The DAG is scheduled to run hourly using `"@hourly"`.
     - `template_searchpath`: This specifies the search path for template files, set to "/tmp."
     - `max_active_runs`: It's set to 1, indicating that only one instance of this DAG can run concurrently.

**Define Functions**:
   - Two Python functions are defined within the DAG:
     - `_get_trending_movies`: This function fetches information about trending movies from the TMDb API. It uses the API token stored in an Airflow Variable, makes an API request, and saves the response data to a JSON file to the `tmp/data/{current_date}` directory.
     - `_get_posters`: This function is responsible for downloading movie posters for the trending movies. It reads the JSON file generated by `_get_trending_movies` and downloads the poster images to the `tmp/img/{current_date}` directory.

**Define Tasks**:
   - Three Airflow tasks are created within the DAG:
     - `create_folders`: A BashOperator task that executes a shell script named "make_folders.sh." This bash script creates folders in `tmp` directory for storing data and images.
     - `get_movies`: A PythonOperator task that executes the `_get_trending_movies` function, which fetches movie data from the TMDb API and saves it to a JSON file.
     - `get_posters`: Another PythonOperator task that executes the `_get_posters` function, which downloads movie posters based on the data in the JSON file.

**Task Dependencies**:
   - The `>>` operator is used to define the execution order (task dependencies) within the DAG. The tasks are sequenced as follows:
     - `create_folders` >> `get_movies` >> `get_posters`

The output of this DAG are the trending movies and their posters from the TMDb API. It runs on an weekly schedule and stores the data and images in specified folders.