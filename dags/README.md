## Learn Airflow

### Dags folder

Inside this folder you will find some sample dags made to be executed in te Airflow server, you just need to create a conda environment following the steps detailed in the main [README file](https://github.com/fvgm-spec/learn-airflow/blob/main/README.md) of this repo.

### **hello_world_dag.py**

This dag corresponds to the `hello_world` sample in every programming languague, which is the first code executed to start learning the basics.

Here a basic DAG is created that runs in an hourly basis by executing a `heloworld` function.

### **reading_pageviews.py**

This [sample DAG](https://github.com/BasPH/data-pipelines-with-apache-airflow/blob/master/chapter04/dags/listing_4_20.py) is introduced in the book from *Bas Hareslak - Data Pipelines with Airflow* aka. The Airflow Bible.

This dag basically performs the following steps:

This Apache Airflow DAG, named "read_wiki_pages," is designed to automate the process of fetching pageview data from Wikipedia, processing it, and then storing it in a PostgreSQL database. Here's a step-by-step explanation of the DAG's functionality:

1. **Import Necessary Modules and Libraries**:
   - The code starts by importing various modules and libraries, including modules for making HTTP requests (`urllib.request`), date handling (`airflow.utils.dates`), and Airflow components (`DAG`, `PythonOperator`, `BashOperator`, `PostgresOperator`).

2. **Creating an Airflow DAG**:
   - A DAG object is created with several attributes and settings:
     - `dag_id`: This is a unique identifier for the DAG, set as "read_wiki_pages."
     - `start_date`: The DAG's start date is set to one day ago from the current date using `airflow.utils.dates.days_ago(1)`.
     - `schedule_interval`: The DAG is scheduled to run hourly using `"@hourly"`.
     - `template_searchpath`: This specifies the search path for template files. In this case, it's set to "/tmp."
     - `max_active_runs`: It's set to 1, indicating that only one instance of this DAG can run concurrently.

3. **Defining Functions**:
   - Two Python functions are defined within the DAG:
     - `_get_data`: This function retrieves data from a URL that corresponds to Wikipedia pageviews for a specific date and time. It constructs the URL dynamically based on input arguments and saves the data to a specified output file.
     - `_fetch_pageviews`: This function processes the downloaded pageview data, extracts relevant information, and prepares SQL queries to insert the data into a PostgreSQL database.

4. **Creating Tasks**:
   - Several Airflow tasks are created within the DAG:
     - `get_data`: A PythonOperator task that executes the `_get_data` function. It retrieves pageview data from a URL based on the execution date and saves it to a gzip-compressed file.
     - `extract_gz`: A BashOperator task that runs the "gunzip" command to decompress the downloaded data file.
     - `fetch_pageviews`: Another PythonOperator task that executes the `_fetch_pageviews` function. It processes the extracted pageview data and prepares SQL queries.
     - `write_to_postgres`: A PostgresOperator task that writes data to a PostgreSQL database. It uses the SQL queries generated by the `_fetch_pageviews` function.

5. **Task Dependencies**:
   - The `>>` operator is used to define the execution order (task dependencies) within the DAG. The tasks are sequenced as follows:
     - `get_data` >> `extract_gz` >> `fetch_pageviews` >> `write_to_postgres`

This DAG is designed to automate the process of fetching, processing, and storing Wikipedia pageview data, making it a useful tool for data pipeline automation in an Airflow environment.

### **getting_movies.py**


